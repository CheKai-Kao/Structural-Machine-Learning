{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94e9ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### package\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08800e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "### count params\n",
    "import operator\n",
    "from functools import reduce\n",
    "\n",
    "# print the number of parameters\n",
    "def count_params(model):\n",
    "    c = 0\n",
    "    for p in list(model.parameters()):\n",
    "        c += reduce(operator.mul, list(p.size()))\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a0f5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### read data function\n",
    "def read_one_year(year, c_idx_list, timestep, datatype, \n",
    "                  height=157, width=103, keep_missing_hour=True):  \n",
    "    start_time = datetime(year, 1, 1, 0)\n",
    "    end_time = datetime(year + 1, 1, 1, 0)\n",
    "    delta = timedelta(hours=timestep)\n",
    "\n",
    "    data_tensor = []  \n",
    "    mask_tensor = []\n",
    "    current_time = start_time\n",
    "    while current_time < end_time:\n",
    "        timestamp = current_time.strftime('%Y%m%d%H')\n",
    "        path = construct_file_path(year, timestamp, datatype)   # 建立檔案完整路徑\n",
    "        grid_data = read_data_from_file(path, c_idx_list)   # 讀取檔案\n",
    "\n",
    "        if grid_data is not None:\n",
    "            valid_values = grid_data[torch.isfinite(grid_data)]    # 回傳既不是 NaN 或 ±Inf 的值\n",
    "            if valid_values.numel() > 0 and torch.all(valid_values == valid_values[0]):   # 如果tensor的數值都一樣，視為異常數據\n",
    "                data_tensor.append(torch.full((len(c_idx_list), height, width), float('nan')))\n",
    "                mask_tensor.append(0.0)\n",
    "                print(f\"All valid values are the same in file: {path}. Replacing with NaN tensor.\")\n",
    "            else:\n",
    "                data_tensor.append(grid_data)\n",
    "                mask_tensor.append(1.0)\n",
    "        else:\n",
    "            if keep_missing_hour:\n",
    "                data_tensor.append(torch.full((len(c_idx_list), height, width), float('nan')))\n",
    "                mask_tensor.append(0.0)\n",
    "\n",
    "        current_time += delta\n",
    "\n",
    "    return torch.stack(data_tensor), torch.tensor(mask_tensor)    # [total hours, C, H, W]\n",
    "\n",
    "\n",
    "def construct_file_path(year, timestamp, mode):\n",
    "    if mode == \"surfgrid\": \n",
    "        return f\"../PT_grid_data_{year}(hour)/surfgrid_RCEC_{timestamp}.pt\"\n",
    "    elif mode == \"obs\":\n",
    "        return f\"../PT_observation_{year}(hour)/observation_{timestamp}.pt\"\n",
    "    else:\n",
    "        raise ValueError(\"Invalid mode! Must be 'surfgrid' or 'obs'.\")\n",
    "\n",
    "\n",
    "def read_data_from_file(path, c_idx):\n",
    "    if os.path.exists(path):\n",
    "        return torch.load(path, weights_only=True)[c_idx, ...]  # [C_selected, H, W]\n",
    "    else:\n",
    "        print(f\"⚠️ Missing file: {path}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def read_data(year_list: list,c_idx_list: list, timestep: int, \n",
    "              datatype: str, keep_missing_hour=True):\n",
    "    \"\"\"\n",
    "    載入多個年份的資料，將其串接成一個大tensor，支援多通道輸入。\n",
    "    return：data_tensor: [8760, C, H, W], mask_tensor: [8760]\n",
    "    \"\"\"\n",
    "    all_data_tensors = []\n",
    "    all_mask_tensors = []   # 遺失或是資料不正確為0\n",
    "\n",
    "    for year in year_list:\n",
    "        data_tensor, mask_tensor = read_one_year(\n",
    "            int(year), c_idx_list, int(timestep), datatype, keep_missing_hour=keep_missing_hour\n",
    "        )\n",
    "        all_data_tensors.append(data_tensor)\n",
    "        all_mask_tensors.append(mask_tensor)\n",
    "\n",
    "    return torch.cat(all_data_tensors), torch.cat(all_mask_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f36666",
   "metadata": {},
   "outputs": [],
   "source": [
    "### loading data tensor\n",
    "train_data_tensor, train_mask_tensor = read_data(\n",
    "    year_list=[2020, 2021, 2022],\n",
    "    c_idx_list=[3, 5, 6, 7, 8], # (PM25, windspeed, winddir, K, humidity%)\n",
    "    timestep=1,\n",
    "    datatype=\"surfgrid\",\n",
    ")  # data_tensor: [T_total, C, H, W], mask_tensor: [T_total]\n",
    "\n",
    "val_data_tensor, val_mask_tensor = read_data(\n",
    "    year_list=[2023],\n",
    "    c_idx_list=[3, 5, 6, 7, 8],\n",
    "    timestep=1,\n",
    "    datatype=\"surfgrid\",\n",
    ")  # data_tensor: [T_total, C, H, W], mask_tensor: [T_total]\n",
    "\n",
    "print(train_data_tensor.shape)\n",
    "print(val_data_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e164b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### convert wind to u,v \n",
    "def convert_wind_to_uv(data_tensor: torch.Tensor, speed_idx: int, dir_idx: int):\n",
    "    speed = data_tensor[:, speed_idx]\n",
    "    direction = data_tensor[:, dir_idx]\n",
    "\n",
    "    if torch.any(direction < 0):\n",
    "        print(\"Skip: already converted to u/v.\")\n",
    "        return data_tensor\n",
    "\n",
    "    theta_rad = direction * torch.pi / 180.0\n",
    "    u = -speed * torch.sin(theta_rad)\n",
    "    v = -speed * torch.cos(theta_rad)\n",
    "\n",
    "    data_tensor[:, speed_idx] = u\n",
    "    data_tensor[:, dir_idx] = v\n",
    "\n",
    "    print(\"Wind converted to u/v.\")\n",
    "    return data_tensor\n",
    "\n",
    "train_data_tensor = convert_wind_to_uv(train_data_tensor, speed_idx=1, dir_idx=2)\n",
    "val_data_tensor = convert_wind_to_uv(val_data_tensor, speed_idx=1, dir_idx=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940bea13",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesWindowDataset(Dataset):\n",
    "    def __init__(self, data_tensor: torch.Tensor, \n",
    "                 mask_tensor: torch.Tensor, \n",
    "                 T_in: int, T: int, \n",
    "                 stride: int = 1,\n",
    "                 add_lonlat: bool = False):\n",
    "        \"\"\"\n",
    "        建立基於 sliding window 的時間序列 Dataset，並選擇性合併經緯度資訊。\n",
    "        - stride: 時間步長 (每幾個時間點取一筆樣本)\n",
    "        \"\"\"\n",
    "        self.data = data_tensor\n",
    "        self.mask = mask_tensor\n",
    "        self.T_in = T_in\n",
    "        self.T = T\n",
    "        self.stride = stride\n",
    "        self.window_size = T_in + T\n",
    "        self.add_lonlat = add_lonlat\n",
    "\n",
    "        if add_lonlat:\n",
    "            fixed_path = \"C:/Users/kevin/Documents/新 科技部計畫/PT_grid_data_2023(hour)/surfgrid_RCEC_2023010100.pt\"\n",
    "            self.grid_lonlat = self._load_and_process_lonlat(fixed_path)\n",
    "        else:\n",
    "            self.grid_lonlat = None\n",
    "\n",
    "        self.valid_indices = self._compute_valid_indices()\n",
    "\n",
    "    def _load_and_process_lonlat(self, path: str) -> torch.Tensor:\n",
    "        latlon_tensor = torch.load(path, weights_only=True)  # [C, H, W]\n",
    "        lat = latlon_tensor[0]\n",
    "        lon = latlon_tensor[1]\n",
    "\n",
    "        def normalize(tensor):\n",
    "            min_val = tensor.min()\n",
    "            max_val = tensor.max()\n",
    "            return 2 * (tensor - min_val) / (max_val - min_val) - 1\n",
    "\n",
    "        lat = normalize(lat)\n",
    "        lon = normalize(lon)\n",
    "        return torch.stack([lon, lat], dim=0)  # [2, H, W]\n",
    "\n",
    "    def _compute_valid_indices(self):\n",
    "        valid_indices = []\n",
    "        total_steps = self.data.shape[0]\n",
    "        for i in range(0, total_steps - self.window_size + 1, self.stride):  \n",
    "            window_mask = self.mask[i: i + self.window_size]\n",
    "            if window_mask.all():\n",
    "                valid_indices.append(i)\n",
    "        return valid_indices\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.valid_indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        start_idx = self.valid_indices[idx]\n",
    "        end_idx = start_idx + self.window_size\n",
    "        window = self.data[start_idx:end_idx]\n",
    "\n",
    "        X_pm25 = window[:self.T_in, :1]     # PM25\n",
    "        condition = window[self.T_in:, 1:]\n",
    "        X = torch.cat([X_pm25, condition], dim=1)\n",
    "        y = window[self.T_in:, :1]     # PM25\n",
    "\n",
    "        T_in, C_in = X.shape[0], X.shape[1]\n",
    "        X = X.reshape(T_in * C_in, *X.shape[2:])\n",
    "\n",
    "        T_out, C_out = y.shape[0], y.shape[1]\n",
    "        y = y.reshape(T_out * C_out, *y.shape[2:])\n",
    "\n",
    "        if self.add_lonlat and self.grid_lonlat is not None:\n",
    "            X = torch.cat([X, self.grid_lonlat], dim=0)\n",
    "\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee98e74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### build dataset/dataloader\n",
    "T_in = 1\n",
    "T = 1\n",
    "stride = 2\n",
    "\n",
    "train_dataset = TimeSeriesWindowDataset(train_data_tensor, train_mask_tensor, T_in, T, stride, add_lonlat=True)\n",
    "val_dataset = TimeSeriesWindowDataset(val_data_tensor, val_mask_tensor, T_in, T, stride, add_lonlat=True)\n",
    "\n",
    "n_train = len(train_dataset)\n",
    "n_val = len(val_dataset)\n",
    "print(\"訓練資料數量: \", n_train)\n",
    "print(\"測試資料數量: \", n_val)\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84820cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### check dataloader\n",
    "def check_dataloader_nan_and_range(dataloader, name=\"\"):\n",
    "    print(f\"\\n📦 檢查 DataLoader：{name}\")\n",
    "    for i, (batch_X, batch_y) in enumerate(dataloader):\n",
    "        nan_X = torch.isnan(batch_X).any().item()\n",
    "        nan_y = torch.isnan(batch_y).any().item()\n",
    "\n",
    "        print(f\"🔁 Batch {i+1}:\")\n",
    "        \n",
    "        print(f\" - X shape: {batch_X.shape}, 含 NaN: {nan_X}\")\n",
    "        if not nan_X:\n",
    "            #  batch_X 形狀為 (B, T*C, H, W)\n",
    "            C = batch_X.shape[1]\n",
    "            for c in range(C):\n",
    "                x_c = batch_X[:, c, :, :].flatten()\n",
    "                min_val = x_c.min().item()\n",
    "                max_val = x_c.max().item()\n",
    "                median_val = x_c.median().item()\n",
    "                print(f\"   - 通道 {c}: min={min_val:.2f}, median={median_val:.2f}, max={max_val:.2f}\")\n",
    "        else:\n",
    "            print(\" - X 數值範圍: (含 NaN，略過 per-channel 統計)\")\n",
    "\n",
    "        print(f\" - y shape: {batch_y.shape}, 含 NaN: {nan_y}\")\n",
    "        if not nan_y:\n",
    "            #  batch_y 形狀為 (B, T*C, H, W)\n",
    "            C = batch_y.shape[1]\n",
    "            for c in range(C):\n",
    "                y_c = batch_y[:, c, :, :].flatten()\n",
    "                min_val = y_c.min().item()\n",
    "                max_val = y_c.max().item()\n",
    "                median_val = y_c.median().item()\n",
    "                print(f\"   - 通道 {c}: min={min_val:.2f}, median={median_val:.2f}, max={max_val:.2f}\")\n",
    "        else:\n",
    "            print(\" - y 數值範圍: (含 NaN，略過 per-channel 統計)\")\n",
    "\n",
    "# ✅ 執行檢查\n",
    "check_dataloader_nan_and_range(train_loader, \"Train\")\n",
    "check_dataloader_nan_and_range(val_loader, \"Validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a663cf79",
   "metadata": {},
   "outputs": [],
   "source": [
    "### build model\n",
    "from neuralop.models import TFNO\n",
    "model = TFNO(\n",
    "    n_modes=(28, 18),     # 不能設太高，很容易overvitting\n",
    "    hidden_channels=28,\n",
    "    in_channels=7, \n",
    "    out_channels=1,\n",
    "    positional_embedding=None,\n",
    "    domain_padding=0.1,\n",
    "    n_layers=3,\n",
    "    factorization='tucker',\n",
    "    implementation='factorized',\n",
    "    rank=0.02    # 可以設低\n",
    ")\n",
    "model.cuda()\n",
    "print(\"模型參數量: \", count_params(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08d4d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "### model summary\n",
    "from torchinfo import summary\n",
    "\n",
    "print(summary(model, input_size=(2, 7, 157, 103)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489c3ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### build optimizer\n",
    "learning_rate = 1e-3\n",
    "epochs = 100\n",
    "iterations = epochs*(n_train//batch_size)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=iterations, eta_min=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a8133b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### record loss / best val loss\n",
    "best_val_loss = float('inf')\n",
    "train_loss_history = []\n",
    "val_loss_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4995ac60",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training\n",
    "from timeit import default_timer as timer\n",
    "from datetime import timedelta\n",
    "from neuralop.losses import LpLoss\n",
    "\n",
    "model_name = 'best_FNO_test'\n",
    "loss_csv_path = 'loss_history_FNO_test.csv'\n",
    "device = \"cuda\"\n",
    "myloss = nn.MSELoss()    \n",
    "#myloss = LpLoss(d=2, p=1) \n",
    "# def myloss(pred, y):\n",
    "#     # 建立 mask，僅選擇 y > 35 的位置\n",
    "#     mask = y > 12\n",
    "    \n",
    "#     # 若沒有符合條件的資料點，避免除以 0\n",
    "#     if mask.sum() == 0:\n",
    "#         return torch.tensor(0.0, device=y.device)\n",
    "    \n",
    "#     # 選取符合條件的 pred 與 y，計算 RMSE\n",
    "#     mse = F.mse_loss(pred[mask], y[mask], reduction='mean')\n",
    "#     rmse = torch.sqrt(mse)\n",
    "#     return rmse\n",
    "\n",
    "# 計時開始\n",
    "start_time = timer()\n",
    "\n",
    "# Early stopping 參數\n",
    "patience = 20\n",
    "early_stop_counter = 0\n",
    "\n",
    "for ep in range(epochs):\n",
    "    # ========== 訓練階段 ==========\n",
    "    model.train()\n",
    "    train_loss_accum = 0.0\n",
    "    train_samples = 0\n",
    "\n",
    "    for x, y in tqdm(train_loader, desc=f\"Training Epoch {ep+1}/{epochs}\"):\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        pred = model(x)\n",
    "        # loss = myloss(pred, y)\n",
    "        T0_pm25 = x[:, :1]\n",
    "        loss = myloss(pred+T0_pm25, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        train_loss_accum += loss.item() * x.size(0)\n",
    "        train_samples += x.size(0)\n",
    "\n",
    "    train_loss = train_loss_accum / train_samples\n",
    "    train_loss_history.append(train_loss)\n",
    "\n",
    "    print(f\"Train Loss: {train_loss:.6f}\")\n",
    "\n",
    "    # ========== 驗證階段 ==========\n",
    "    model.eval()\n",
    "    val_loss_accum = 0.0\n",
    "    val_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in tqdm(val_loader, desc=f\"Validation Epoch {ep+1}/{epochs}\"):\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            pred = model(x)\n",
    "            #loss = myloss(pred, y)\n",
    "            T0_pm25 = x[:, :1]\n",
    "            loss = myloss(pred+T0_pm25, y)\n",
    "\n",
    "            val_loss_accum += loss.item() * x.size(0)\n",
    "            val_samples += x.size(0)\n",
    "\n",
    "    val_loss = val_loss_accum / val_samples\n",
    "    val_loss_history.append(val_loss)\n",
    "\n",
    "    print(f\"Valid Loss: {val_loss:.6f}\")\n",
    "\n",
    "    # 檢查是否為最佳模型\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        early_stop_counter = 0  # 重置 counter\n",
    "        model.save_checkpoint(save_folder='checkpoint', save_name=model_name)\n",
    "        print(f\"💾 Best model saved (val_loss={val_loss:.6f})\")\n",
    "    else:\n",
    "        early_stop_counter += 1\n",
    "        print(f\"⚠️ No improvement. Early stop counter: {early_stop_counter}/{patience}\")\n",
    "\n",
    "    # 每10個epoch儲存loss到CSV檔案\n",
    "    if (ep + 1) % 10 == 0:\n",
    "        # 創建DataFrame\n",
    "        loss_df = pd.DataFrame({\n",
    "            'Epoch': range(1, len(train_loss_history) + 1),\n",
    "            'Train_Loss': train_loss_history,\n",
    "            'Val_Loss': val_loss_history\n",
    "        })\n",
    "        \n",
    "        # 儲存到CSV\n",
    "        loss_df.to_csv(loss_csv_path, index=False)\n",
    "        print(f\"📊 Save loss history (up to epoch {ep+1})\")\n",
    "\n",
    "    # 若超過耐心次數則提前停止訓練\n",
    "    if early_stop_counter >= patience:\n",
    "        print(f\"🛑 Early stopping triggered at epoch {ep+1}.\")\n",
    "        break\n",
    "\n",
    "    # 結束一個 epoch\n",
    "    print('-'*50)\n",
    "\n",
    "# 計時結束\n",
    "end_time = timer()\n",
    "elapsed_time = end_time - start_time\n",
    "formatted_time = str(timedelta(seconds=int(elapsed_time)))\n",
    "print(f\"⏱️ Total training time: {formatted_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127911a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(os.path.dirname(loss_csv_path), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7e16f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_df = pd.DataFrame({\n",
    "    'epoch': list(range(1, len(train_loss_history)+1)),\n",
    "    'train_loss': train_loss_history,\n",
    "    'val_loss': val_loss_history\n",
    "})\n",
    "loss_df.to_csv('./project_structural_machine_learning/FNO_H38_W26_loss_history.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myPytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
